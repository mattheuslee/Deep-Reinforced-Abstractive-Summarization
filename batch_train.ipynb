{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u4Zzmw6W4aOi"
   },
   "source": [
    "# Toggle between CPU and GPU(CUDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "y6nQ98Yv4ZmV"
   },
   "outputs": [],
   "source": [
    "USE_CUDA = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V8ZHf_q3s9L-"
   },
   "source": [
    "# Install Python libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 255,
     "output_extras": [
      {
       "item_id": 6
      },
      {
       "item_id": 7
      }
     ]
    },
    "colab_type": "code",
    "collapsed": true,
    "executionInfo": {
     "elapsed": 6277,
     "status": "ok",
     "timestamp": 1521988257227,
     "user": {
      "displayName": "Mattheus Lee",
      "photoUrl": "//lh4.googleusercontent.com/-73pDOgoEzIs/AAAAAAAAAAI/AAAAAAAAAEQ/awPuz3WAVvI/s50-c-k-no/photo.jpg",
      "userId": "106415181672432219840"
     },
     "user_tz": -480
    },
    "id": "B7K69ZG2vYP1",
    "outputId": "24565ba3-9245-4f42-b5da-c5d2d3aeeac6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in /usr/local/lib/python2.7/dist-packages\r\n",
      "Requirement already satisfied: torchtext in /usr/local/lib/python2.7/dist-packages\r\n",
      "Requirement already satisfied: requests in /usr/local/lib/python2.7/dist-packages (from torchtext)\n",
      "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python2.7/dist-packages (from requests->torchtext)\n",
      "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python2.7/dist-packages (from requests->torchtext)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python2.7/dist-packages (from requests->torchtext)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python2.7/dist-packages (from requests->torchtext)\n",
      "Requirement already satisfied: gputil in /usr/local/lib/python2.7/dist-packages\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python2.7/dist-packages (from gputil)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python2.7/dist-packages\n",
      "Requirement already satisfied: humanize in /usr/local/lib/python2.7/dist-packages\n",
      "[nltk_data] Downloading package punkt to /content/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# http://pytorch.org/\n",
    "from os import path\n",
    "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
    "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
    "\n",
    "accelerator = 'cu80' if path.exists('/opt/bin/nvidia-smi') else 'cpu'\n",
    "\n",
    "!pip install -q http://download.pytorch.org/whl/{accelerator}/torch-0.3.0.post4-{platform}-linux_x86_64.whl torchvision\n",
    "!pip install tqdm torchtext\n",
    "!pip install gputil\n",
    "!pip install psutil\n",
    "!pip install humanize\n",
    "import nltk\n",
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "O7InUlFLtEQx"
   },
   "source": [
    "## Download additional libraries and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 629,
     "output_extras": [
      {
       "item_id": 4
      }
     ]
    },
    "colab_type": "code",
    "collapsed": true,
    "executionInfo": {
     "elapsed": 1864,
     "status": "ok",
     "timestamp": 1521988269779,
     "user": {
      "displayName": "Mattheus Lee",
      "photoUrl": "//lh4.googleusercontent.com/-73pDOgoEzIs/AAAAAAAAAAI/AAAAAAAAAEQ/awPuz3WAVvI/s50-c-k-no/photo.jpg",
      "userId": "106415181672432219840"
     },
     "user_tz": -480
    },
    "id": "6Q8uBFrnvtCF",
    "outputId": "e1c1fc69-f3e3-4be8-c50c-a22b4674517a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2018-03-25 14:31:07--  https://raw.githubusercontent.com/mattheuslee/Deep-Reinforced-Abstractive-Summarization/master/attention.py\r\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\r\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 6831 (6.7K) [text/plain]\n",
      "Saving to: ‘attention.py’\n",
      "\n",
      "attention.py        100%[===================>]   6.67K  --.-KB/s    in 0s      \n",
      "\n",
      "2018-03-25 14:31:07 (94.5 MB/s) - ‘attention.py’ saved [6831/6831]\n",
      "\n",
      "rm: cannot remove 'attention.pyc': No such file or directory\n",
      "--2018-03-25 14:31:07--  https://raw.githubusercontent.com/mattheuslee/Deep-Reinforced-Abstractive-Summarization/master/model.py\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 7513 (7.3K) [text/plain]\n",
      "Saving to: ‘model.py’\n",
      "\n",
      "model.py            100%[===================>]   7.34K  --.-KB/s    in 0s      \n",
      "\n",
      "2018-03-25 14:31:07 (136 MB/s) - ‘model.py’ saved [7513/7513]\n",
      "\n",
      "rm: cannot remove 'model.pyc': No such file or directory\n",
      "--2018-03-25 14:31:07--  https://raw.githubusercontent.com/mattheuslee/Deep-Reinforced-Abstractive-Summarization/master/rouge.py\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1614 (1.6K) [text/plain]\n",
      "Saving to: ‘rouge.py’\n",
      "\n",
      "rouge.py            100%[===================>]   1.58K  --.-KB/s    in 0s      \n",
      "\n",
      "2018-03-25 14:31:07 (376 MB/s) - ‘rouge.py’ saved [1614/1614]\n",
      "\n",
      "rm: cannot remove 'rouge.pyc': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!wget -O attention.py https://raw.githubusercontent.com/mattheuslee/Deep-Reinforced-Abstractive-Summarization/master/attention.py && rm attention.pyc\n",
    "!wget -O model.py https://raw.githubusercontent.com/mattheuslee/Deep-Reinforced-Abstractive-Summarization/master/model.py && rm model.pyc\n",
    "!wget -O rouge.py https://raw.githubusercontent.com/mattheuslee/Deep-Reinforced-Abstractive-Summarization/master/rouge.py && rm rouge.pyc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 578,
     "output_extras": [
      {
       "item_id": 5
      }
     ]
    },
    "colab_type": "code",
    "collapsed": true,
    "executionInfo": {
     "elapsed": 1810,
     "status": "ok",
     "timestamp": 1521988273865,
     "user": {
      "displayName": "Mattheus Lee",
      "photoUrl": "//lh4.googleusercontent.com/-73pDOgoEzIs/AAAAAAAAAAI/AAAAAAAAAEQ/awPuz3WAVvI/s50-c-k-no/photo.jpg",
      "userId": "106415181672432219840"
     },
     "user_tz": -480
    },
    "id": "SuJKLtZgs330",
    "outputId": "57dbb1fc-6e4e-4bf5-90f6-739e4895fbfe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2018-03-25 14:31:11--  https://raw.githubusercontent.com/mattheuslee/Deep-Reinforced-Abstractive-Summarization/master/batch_100/download.sh\r\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\r\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 234 [text/plain]\n",
      "Saving to: ‘batch_100_download.sh’\n",
      "\n",
      "batch_100_download. 100%[===================>]     234  --.-KB/s    in 0s      \n",
      "\n",
      "2018-03-25 14:31:11 (62.8 MB/s) - ‘batch_100_download.sh’ saved [234/234]\n",
      "\n",
      "--2018-03-25 14:31:11--  https://raw.githubusercontent.com/mattheuslee/Deep-Reinforced-Abstractive-Summarization/master/batch_1000/download.sh\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 236 [text/plain]\n",
      "Saving to: ‘batch_1000_download.sh’\n",
      "\n",
      "batch_1000_download 100%[===================>]     236  --.-KB/s    in 0s      \n",
      "\n",
      "2018-03-25 14:31:11 (58.4 MB/s) - ‘batch_1000_download.sh’ saved [236/236]\n",
      "\n",
      "--2018-03-25 14:31:11--  https://raw.githubusercontent.com/mattheuslee/Deep-Reinforced-Abstractive-Summarization/master/cnn_test_stories.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 39020 (38K) [text/plain]\n",
      "Saving to: ‘cnn_test_stories.txt’\n",
      "\n",
      "cnn_test_stories.tx 100%[===================>]  38.11K  --.-KB/s    in 0.02s   \n",
      "\n",
      "2018-03-25 14:31:12 (2.29 MB/s) - ‘cnn_test_stories.txt’ saved [39020/39020]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget -O batch_100_download.sh https://raw.githubusercontent.com/mattheuslee/Deep-Reinforced-Abstractive-Summarization/master/batch_100/download.sh\n",
    "!wget -O batch_1000_download.sh https://raw.githubusercontent.com/mattheuslee/Deep-Reinforced-Abstractive-Summarization/master/batch_1000/download.sh\n",
    "!wget -O cnn_test_stories.txt https://raw.githubusercontent.com/mattheuslee/Deep-Reinforced-Abstractive-Summarization/master/cnn_test_stories.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KZmAcfWphIK8"
   },
   "source": [
    "Run download script to get batch 100 or 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 34,
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 134805,
     "status": "ok",
     "timestamp": 1521988451689,
     "user": {
      "displayName": "Mattheus Lee",
      "photoUrl": "//lh4.googleusercontent.com/-73pDOgoEzIs/AAAAAAAAAAI/AAAAAAAAAEQ/awPuz3WAVvI/s50-c-k-no/photo.jpg",
      "userId": "106415181672432219840"
     },
     "user_tz": -480
    },
    "id": "GrSfeNZlhH32",
    "outputId": "1d795bc9-04f8-4423-a0cb-351a0b40c382"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘batch_100’: File exists\r\n"
     ]
    }
   ],
   "source": [
    "!bash batch_100_download.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "aZtB32Q0hbGw"
   },
   "outputs": [],
   "source": [
    "!bash batch_1000_download.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6puQH7Jpsd7T"
   },
   "source": [
    "# Run to check GPU stats and usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 340,
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1170,
     "status": "ok",
     "timestamp": 1521988616259,
     "user": {
      "displayName": "Mattheus Lee",
      "photoUrl": "//lh4.googleusercontent.com/-73pDOgoEzIs/AAAAAAAAAAI/AAAAAAAAAEQ/awPuz3WAVvI/s50-c-k-no/photo.jpg",
      "userId": "106415181672432219840"
     },
     "user_tz": -480
    },
    "id": "JlqRoS6tMrQr",
    "outputId": "378ff265-c48f-4293-862c-9e6da33b17ed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ln: cannot remove ‘/usr/bin/nvidia-smi’: Permission denied\r\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Process' object has no attribute 'memory_info'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-1d84f53d33a7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mgpu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGPUs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mprocess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpsutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mProcess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetpid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Gen RAM Free: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mhumanize\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnaturalsize\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mpsutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvirtual_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mavailable\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\" | Proc size: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mhumanize\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnaturalsize\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgpu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemoryFree\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgpu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemoryUsed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgpu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemoryUtil\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgpu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemoryTotal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mu'nvidia-smi'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Process' object has no attribute 'memory_info'"
     ]
    }
   ],
   "source": [
    "# memory footprint support libraries/code\n",
    "!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
    "import psutil\n",
    "import humanize\n",
    "import os\n",
    "import GPUtil as GPU\n",
    "GPUs = GPU.getGPUs()\n",
    "# XXX: only one GPU on Colab and isn’t guaranteed\n",
    "gpu = GPUs[0]\n",
    "process = psutil.Process(os.getpid())\n",
    "print(\"Gen RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ), \" | Proc size: \" + humanize.naturalsize( process.memory_info().rss))\n",
    "print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lH5n5S1LtYmy"
   },
   "source": [
    "# Main Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "n65JdL8fvSMY"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import nltk\n",
    "from torchtext.data import Field,BucketIterator, TabularDataset\n",
    "from tqdm import tqdm, trange, tnrange, tqdm_notebook\n",
    "import numpy as np\n",
    "from __future__ import print_function\n",
    "\n",
    "from model import Encoder, Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "g_UJhhA7wDZw"
   },
   "outputs": [],
   "source": [
    "TEXT = Field(tokenize=nltk.word_tokenize,use_vocab=True,lower=True, include_lengths=True, batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "8hHHGXUDwGYf"
   },
   "outputs": [],
   "source": [
    "MINI_BATCH_SIZE = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qVI68sfUtdy_"
   },
   "source": [
    "**Change path if using different data file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 68,
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1022,
     "status": "ok",
     "timestamp": 1521989736652,
     "user": {
      "displayName": "Mattheus Lee",
      "photoUrl": "//lh4.googleusercontent.com/-73pDOgoEzIs/AAAAAAAAAAI/AAAAAAAAAEQ/awPuz3WAVvI/s50-c-k-no/photo.jpg",
      "userId": "106415181672432219840"
     },
     "user_tz": -480
    },
    "id": "PniQE5rkYd5O",
    "outputId": "23ac71d8-643b-4339-d17f-5bdccd77256e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention.py   batch_1000\t       cnn_test_stories.txt  model.pyc\r\n",
      "attention.pyc  batch_1000_download.sh  datalab\t\t     nltk_data\r\n",
      "batch_100      batch_100_download.sh   model.py\t\t     rouge.py\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 102,
     "output_extras": [
      {
       "item_id": 465
      },
      {
       "item_id": 466
      },
      {
       "item_id": 467
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 354574,
     "status": "ok",
     "timestamp": 1521990094533,
     "user": {
      "displayName": "Mattheus Lee",
      "photoUrl": "//lh4.googleusercontent.com/-73pDOgoEzIs/AAAAAAAAAAI/AAAAAAAAAEQ/awPuz3WAVvI/s50-c-k-no/photo.jpg",
      "userId": "106415181672432219840"
     },
     "user_tz": -480
    },
    "id": "BH_Lx1_EwHZo",
    "outputId": "f97de16b-be90-4faa-ad5c-10951e0aa90f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading batches: 100%|██████████| 800/800 [08:05<00:00,  1.65batch/s]\n",
      "Batch training dataloaders: 100%|██████████| 800/800 [00:00<00:00, 42399.90batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 188274\n",
      "Number of training examples: 79903\n",
      "Number of testing stories: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE  = 100\n",
    "NUM_BATCHES = 800\n",
    "\n",
    "train_data_batches = []\n",
    "for batch_idx in trange(NUM_BATCHES, desc = \"Loading batches\", unit = \"batch\"):\n",
    "    train_data_batches.append(TabularDataset(path = \"batch_{}/cnn_stories_80000_{}.txt\".format(BATCH_SIZE, batch_idx),\n",
    "                                             format = \"tsv\",\n",
    "                                             fields = [(\"input\", TEXT), (\"target\", TEXT)]))\n",
    "\n",
    "test_data  = TabularDataset(path=\"cnn_test_stories.txt\",\n",
    "                            format='tsv',\n",
    "                            fields=[('input',TEXT),('target',TEXT)])\n",
    "\n",
    "TEXT.build_vocab(test_data, *train_data_batches, min_freq=2)\n",
    "tqdm.write(\"Vocabulary size: {}\".format(len(TEXT.vocab)))\n",
    "\n",
    "batch_train_loaders = []\n",
    "for batch_idx in trange(NUM_BATCHES, desc = \"Batch training dataloaders\", unit = \"batch\"):\n",
    "    batch_train_loaders.append(BucketIterator(train_data_batches[batch_idx], \n",
    "                               batch_size = MINI_BATCH_SIZE,\n",
    "                               device = -1,\n",
    "                               sort_key = lambda x: len(x.input),\n",
    "                               sort_within_batch = True,\n",
    "                               repeat = False,\n",
    "                               shuffle = True))\n",
    "\n",
    "test_loader  = BucketIterator(test_data,\n",
    "                              batch_size = 1, \n",
    "                              device = -1,\n",
    "                              sort_key = lambda x: len(x.input),\n",
    "                              sort_within_batch = True,\n",
    "                              repeat = False,\n",
    "                              shuffle = True)\n",
    "\n",
    "# May be slightly less due to skipping empty stories\n",
    "num_training_examples = np.sum([len(t) for t in train_data_batches])\n",
    "tqdm.write(\"Number of training examples: {}\".format(num_training_examples))\n",
    "tqdm.write(\"Number of testing stories: {}\".format(len(test_data)))\n",
    "\n",
    "HIDDEN = 200\n",
    "EMBED = 100\n",
    "VOCAB_SIZE = len(TEXT.vocab)\n",
    "LR = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "kM0lciHpZcyq"
   },
   "outputs": [],
   "source": [
    "encoder = Encoder(VOCAB_SIZE,EMBED,HIDDEN,bidirec=True)\n",
    "decoder = Decoder(VOCAB_SIZE,EMBED,HIDDEN*2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3gtaqdgnQatb"
   },
   "source": [
    "### Load/save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 299,
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 770,
     "status": "error",
     "timestamp": 1521990201367,
     "user": {
      "displayName": "Mattheus Lee",
      "photoUrl": "//lh4.googleusercontent.com/-73pDOgoEzIs/AAAAAAAAAAI/AAAAAAAAAEQ/awPuz3WAVvI/s50-c-k-no/photo.jpg",
      "userId": "106415181672432219840"
     },
     "user_tz": -480
    },
    "id": "AJInFa63Qd3L",
    "outputId": "23730d26-0c91-433a-ddf8-d52778f88204"
   },
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mIOError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-bf55e36a8493>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./encoder.model\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./decoder.model\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/torch/serialization.pyc\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module)\u001b[0m\n\u001b[1;32m    257\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpathlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m         \u001b[0mnew_fd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 259\u001b[0;31m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    260\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: [Errno 2] No such file or directory: './encoder.model'"
     ]
    }
   ],
   "source": [
    "encoder.load_state_dict(torch.load(\"./encoder.model\"))\n",
    "decoder.load_state_dict(torch.load(\"./decoder.model\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "1n3k9hfsQisO"
   },
   "outputs": [],
   "source": [
    "torch.save(encoder.state_dict(), \"./encoder.model\")\n",
    "torch.save(decoder.state_dict(), \"./decoder.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 34,
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5394,
     "status": "ok",
     "timestamp": 1521990393799,
     "user": {
      "displayName": "Mattheus Lee",
      "photoUrl": "//lh4.googleusercontent.com/-73pDOgoEzIs/AAAAAAAAAAI/AAAAAAAAAEQ/awPuz3WAVvI/s50-c-k-no/photo.jpg",
      "userId": "106415181672432219840"
     },
     "user_tz": -480
    },
    "id": "hNHLY6lmQX69",
    "outputId": "3d218bd2-eea2-4051-9c7d-3e471d84434a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA\n"
     ]
    }
   ],
   "source": [
    "if USE_CUDA:\n",
    "    tqdm.write(\"Using CUDA\")\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        print(\"Using %d devices\" % (torch.cuda.device_count()))\n",
    "        encoder = nn.DataParallel(encoder)\n",
    "        decoder = nn.DataParallel(decoder)\n",
    "    encoder = encoder.cuda()\n",
    "    decoder = decoder.cuda()\n",
    "decoder.embedding = encoder.embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "dWlm1FyiZhE9"
   },
   "outputs": [],
   "source": [
    "loss_function = nn.CrossEntropyLoss(ignore_index=TEXT.vocab.stoi['<pad>'])\n",
    "enc_optim = optim.Adam(encoder.parameters(),lr=LR)\n",
    "dec_optim = optim.Adam(decoder.parameters(),lr=LR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4pQocA_fyO4R"
   },
   "source": [
    "### Run training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 588,
     "output_extras": [
      {
       "item_id": 3
      },
      {
       "item_id": 4
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1874,
     "status": "error",
     "timestamp": 1521990412963,
     "user": {
      "displayName": "Mattheus Lee",
      "photoUrl": "//lh4.googleusercontent.com/-73pDOgoEzIs/AAAAAAAAAAI/AAAAAAAAAEQ/awPuz3WAVvI/s50-c-k-no/photo.jpg",
      "userId": "106415181672432219840"
     },
     "user_tz": -480
    },
    "id": "zD33-15GZkD_",
    "outputId": "14591af7-abcf-4db8-fb16-7e4daf03fa55"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   0%|          | 0/1 [00:00<?, ?epoch/s]\n",
      "Batches:   0%|          | 0/800 [00:00<?, ?batch/s]\u001b[A\n",
      "\n",
      "Minibatches:   0%|          | 0/100 [00:00<?, ?minibatch/s]\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "cuda runtime error (2) : out of memory at /pytorch/torch/lib/THC/generic/THCTensorMath.cu:35",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-5b865717dd7a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0mtotal_squared_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0mnum_batches\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m             \u001b[0menc_optim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0mdec_optim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/torch/autograd/variable.pyc\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m    165\u001b[0m                 \u001b[0mVariable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m         \"\"\"\n\u001b[0;32m--> 167\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/torch/autograd/__init__.pyc\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(variables, grad_variables, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m---> 99\u001b[0;31m         variables, grad_variables, retain_graph)\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: cuda runtime error (2) : out of memory at /pytorch/torch/lib/THC/generic/THCTensorMath.cu:35"
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS = 1\n",
    "for epoch_idx in trange(NUM_EPOCHS, desc = \"Epochs\", unit = \"epoch\"):\n",
    "    total_loss, total_squared_loss, num_batches = 0.0, 0.0, 0\n",
    "    for batch_train_loader in tqdm(batch_train_loaders, desc = \"Batches\", unit = \"batch\"):\n",
    "        for minibatch in tqdm(batch_train_loader, desc = \"Minibatches\", unit = \"minibatch\"):\n",
    "            inputs,lengths = minibatch.input\n",
    "            targets,_ = minibatch.target\n",
    "            decoding_start = Variable(torch.LongTensor([TEXT.vocab.stoi['<s>']]*targets.size(0))).unsqueeze(1)\n",
    "            if USE_CUDA:\n",
    "                inputs = inputs.cuda()\n",
    "                targets = targets.cuda()\n",
    "                decoding_start = decoding_start.cuda()\n",
    "\n",
    "            encoder.zero_grad()\n",
    "            decoder.zero_grad()\n",
    "            output,hidden = encoder(inputs,lengths.tolist())\n",
    "            score = decoder(decoding_start,hidden,targets.size(1),output,lengths)\n",
    "\n",
    "            loss = loss_function(score,targets.view(-1))\n",
    "            total_loss += loss.data[0]\n",
    "            total_squared_loss += loss.data[0]**2\n",
    "            num_batches += 1\n",
    "            loss.backward()\n",
    "            enc_optim.step()\n",
    "            dec_optim.step()\n",
    "    loss_mean = total_loss / num_batches\n",
    "    loss_variance = (total_squared_loss - (total_loss**2 / num_batches)) / (num_batches - 1)\n",
    "    tqdm.write(\"loss mean: %7.4f, loss variance: %7.4f\" % (loss_mean, loss_variance))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o-spPVC-ySDn"
   },
   "source": [
    "###Run testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 231,
     "output_extras": [
      {}
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 727,
     "status": "error",
     "timestamp": 1521793341757,
     "user": {
      "displayName": "Mattheus Lee",
      "photoUrl": "//lh4.googleusercontent.com/-73pDOgoEzIs/AAAAAAAAAAI/AAAAAAAAAEQ/awPuz3WAVvI/s50-c-k-no/photo.jpg",
      "userId": "106415181672432219840"
     },
     "user_tz": -480
    },
    "id": "V5YFERY3xlmn",
    "outputId": "174cea29-1201-489f-9690-ca5c94edd3a4"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-82793d96e47a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'encoder' is not defined"
     ]
    }
   ],
   "source": [
    "from rouge import ROUGE\n",
    "from __future__ import print_function\n",
    "rouge = ROUGE()\n",
    "\n",
    "print(TEXT.vocab.)\n",
    "\n",
    "def get_string(summary):\n",
    "    result = \"\"\n",
    "    for idx in summary:\n",
    "        if idx == 1: # <E>\n",
    "            break\n",
    "        elif idx == 0: # <unk>\n",
    "            continue\n",
    "        if idx < len(TEXT.vocab.itos):\n",
    "            result += (TEXT.vocab.itos[idx] + \" \")\n",
    "    return result\n",
    "\n",
    "encoder.eval()\n",
    "decoder.eval()\n",
    "print(\"Testing stories\")\n",
    "for batch in test_loader:\n",
    "    inputs, lengths = batch.input\n",
    "    targets, _ = batch.target\n",
    "    decoding_start = Variable(torch.LongTensor([TEXT.vocab.stoi['<s>']]*targets.size(0))).unsqueeze(1)\n",
    "    if USE_CUDA:\n",
    "        inputs = inputs.cuda()\n",
    "        targets = targets.cuda()\n",
    "        decoding_start = decoding_start.cuda()\n",
    "    \n",
    "    output,hidden = encoder(inputs,lengths.tolist())\n",
    "    score = decoder(decoding_start,hidden,targets.size(1),output,lengths)\n",
    "    \n",
    "    reference_summary = targets.data.cpu().numpy()[0]\n",
    "    generated_summary = [np.argmax(word) for word in score.data.cpu().numpy()[0]]\n",
    "    \n",
    "    reference = get_string(reference_summary)\n",
    "    generated = get_string(generated_summary)\n",
    "    \n",
    "    rouge_score = rouge.score(reference, generated)\n",
    "    \n",
    "    print(\"\\nReference summary:\\n{}\".format(reference))\n",
    "    print(\"\\nGenerated summary:\\n{}\".format(generated))\n",
    "    print(\"\\nROUGE score: {}\\n\".format(rouge_score))\n",
    "\n",
    "print(\"Selection of training stories\")\n",
    "NUM_SHOW_TRAINING_STORIES = 10\n",
    "for i, batch in enumerate(train_loader):\n",
    "    if i == NUM_SHOW_TRAINING_STORIES:\n",
    "        break\n",
    "    inputs, lengths = batch.input\n",
    "    targets, _ = batch.target\n",
    "    decoding_start = Variable(torch.LongTensor([TEXT.vocab.stoi['<s>']]*targets.size(0))).unsqueeze(1)\n",
    "    if USE_CUDA:\n",
    "        inputs = inputs.cuda()\n",
    "        targets = targets.cuda()\n",
    "        decoding_start = decoding_start.cuda()\n",
    "    \n",
    "    output,hidden = encoder(inputs,lengths.tolist())\n",
    "    score = decoder(decoding_start,hidden,targets.size(1),output,lengths)\n",
    "    \n",
    "    reference_summary = targets.data.cpu().numpy()[0]\n",
    "    generated_summary = [np.argmax(word) for word in score.data.cpu().numpy()[0]]\n",
    "    \n",
    "    reference = get_string(reference_summary)\n",
    "    generated = get_string(generated_summary)\n",
    "    \n",
    "    rouge_score = rouge.score(reference, generated)\n",
    "    \n",
    "    print(\"\\nReference summary:\\n{}\".format(reference))\n",
    "    print(\"\\nGenerated summary:\\n{}\".format(generated))\n",
    "    print(\"\\nROUGE score: {}\\n\".format(rouge_score))\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "default_view": {},
   "name": "batch_train.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
