{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "batch_train.ipynb",
      "version": "0.3.2",
      "views": {},
      "default_view": {},
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "u4Zzmw6W4aOi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Toggle between CPU and GPU(CUDA)"
      ]
    },
    {
      "metadata": {
        "id": "y6nQ98Yv4ZmV",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "USE_CUDA = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "V8ZHf_q3s9L-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Install Python libraries"
      ]
    },
    {
      "metadata": {
        "id": "B7K69ZG2vYP1",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "output_extras": [
            {
              "item_id": 6
            },
            {
              "item_id": 7
            }
          ],
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "collapsed": true,
        "outputId": "24565ba3-9245-4f42-b5da-c5d2d3aeeac6",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1521988257227,
          "user_tz": -480,
          "elapsed": 6277,
          "user": {
            "displayName": "Mattheus Lee",
            "photoUrl": "//lh4.googleusercontent.com/-73pDOgoEzIs/AAAAAAAAAAI/AAAAAAAAAEQ/awPuz3WAVvI/s50-c-k-no/photo.jpg",
            "userId": "106415181672432219840"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# http://pytorch.org/\n",
        "from os import path\n",
        "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
        "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
        "\n",
        "accelerator = 'cu80' if path.exists('/opt/bin/nvidia-smi') else 'cpu'\n",
        "\n",
        "!pip install -q http://download.pytorch.org/whl/{accelerator}/torch-0.3.0.post4-{platform}-linux_x86_64.whl torchvision\n",
        "!pip install tqdm torchtext\n",
        "!pip install gputil\n",
        "!pip install psutil\n",
        "!pip install humanize\n",
        "import nltk\n",
        "nltk.download(\"punkt\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tqdm in /usr/local/lib/python2.7/dist-packages\r\n",
            "Requirement already satisfied: torchtext in /usr/local/lib/python2.7/dist-packages\r\n",
            "Requirement already satisfied: requests in /usr/local/lib/python2.7/dist-packages (from torchtext)\n",
            "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python2.7/dist-packages (from requests->torchtext)\n",
            "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python2.7/dist-packages (from requests->torchtext)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python2.7/dist-packages (from requests->torchtext)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python2.7/dist-packages (from requests->torchtext)\n",
            "Requirement already satisfied: gputil in /usr/local/lib/python2.7/dist-packages\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python2.7/dist-packages (from gputil)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python2.7/dist-packages\n",
            "Requirement already satisfied: humanize in /usr/local/lib/python2.7/dist-packages\n",
            "[nltk_data] Downloading package punkt to /content/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "metadata": {
        "id": "O7InUlFLtEQx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Download additional libraries and data"
      ]
    },
    {
      "metadata": {
        "id": "6Q8uBFrnvtCF",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "output_extras": [
            {
              "item_id": 4
            }
          ],
          "base_uri": "https://localhost:8080/",
          "height": 629
        },
        "collapsed": true,
        "outputId": "e1c1fc69-f3e3-4be8-c50c-a22b4674517a",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1521988269779,
          "user_tz": -480,
          "elapsed": 1864,
          "user": {
            "displayName": "Mattheus Lee",
            "photoUrl": "//lh4.googleusercontent.com/-73pDOgoEzIs/AAAAAAAAAAI/AAAAAAAAAEQ/awPuz3WAVvI/s50-c-k-no/photo.jpg",
            "userId": "106415181672432219840"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "!wget -O attention.py https://raw.githubusercontent.com/mattheuslee/Deep-Reinforced-Abstractive-Summarization/master/attention.py && rm attention.pyc\n",
        "!wget -O model.py https://raw.githubusercontent.com/mattheuslee/Deep-Reinforced-Abstractive-Summarization/master/model.py && rm model.pyc\n",
        "!wget -O rouge.py https://raw.githubusercontent.com/mattheuslee/Deep-Reinforced-Abstractive-Summarization/master/rouge.py && rm rouge.pyc"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2018-03-25 14:31:07--  https://raw.githubusercontent.com/mattheuslee/Deep-Reinforced-Abstractive-Summarization/master/attention.py\r\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\r\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 6831 (6.7K) [text/plain]\n",
            "Saving to: ‘attention.py’\n",
            "\n",
            "attention.py        100%[===================>]   6.67K  --.-KB/s    in 0s      \n",
            "\n",
            "2018-03-25 14:31:07 (94.5 MB/s) - ‘attention.py’ saved [6831/6831]\n",
            "\n",
            "rm: cannot remove 'attention.pyc': No such file or directory\n",
            "--2018-03-25 14:31:07--  https://raw.githubusercontent.com/mattheuslee/Deep-Reinforced-Abstractive-Summarization/master/model.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 7513 (7.3K) [text/plain]\n",
            "Saving to: ‘model.py’\n",
            "\n",
            "model.py            100%[===================>]   7.34K  --.-KB/s    in 0s      \n",
            "\n",
            "2018-03-25 14:31:07 (136 MB/s) - ‘model.py’ saved [7513/7513]\n",
            "\n",
            "rm: cannot remove 'model.pyc': No such file or directory\n",
            "--2018-03-25 14:31:07--  https://raw.githubusercontent.com/mattheuslee/Deep-Reinforced-Abstractive-Summarization/master/rouge.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1614 (1.6K) [text/plain]\n",
            "Saving to: ‘rouge.py’\n",
            "\n",
            "rouge.py            100%[===================>]   1.58K  --.-KB/s    in 0s      \n",
            "\n",
            "2018-03-25 14:31:07 (376 MB/s) - ‘rouge.py’ saved [1614/1614]\n",
            "\n",
            "rm: cannot remove 'rouge.pyc': No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "SuJKLtZgs330",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "output_extras": [
            {
              "item_id": 5
            }
          ],
          "base_uri": "https://localhost:8080/",
          "height": 578
        },
        "collapsed": true,
        "outputId": "57dbb1fc-6e4e-4bf5-90f6-739e4895fbfe",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1521988273865,
          "user_tz": -480,
          "elapsed": 1810,
          "user": {
            "displayName": "Mattheus Lee",
            "photoUrl": "//lh4.googleusercontent.com/-73pDOgoEzIs/AAAAAAAAAAI/AAAAAAAAAEQ/awPuz3WAVvI/s50-c-k-no/photo.jpg",
            "userId": "106415181672432219840"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "!wget -O batch_100_download.sh https://raw.githubusercontent.com/mattheuslee/Deep-Reinforced-Abstractive-Summarization/master/batch_100/download.sh\n",
        "!wget -O batch_1000_download.sh https://raw.githubusercontent.com/mattheuslee/Deep-Reinforced-Abstractive-Summarization/master/batch_1000/download.sh\n",
        "!wget -O cnn_test_stories.txt https://raw.githubusercontent.com/mattheuslee/Deep-Reinforced-Abstractive-Summarization/master/cnn_test_stories.txt"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2018-03-25 14:31:11--  https://raw.githubusercontent.com/mattheuslee/Deep-Reinforced-Abstractive-Summarization/master/batch_100/download.sh\r\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\r\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 234 [text/plain]\n",
            "Saving to: ‘batch_100_download.sh’\n",
            "\n",
            "batch_100_download. 100%[===================>]     234  --.-KB/s    in 0s      \n",
            "\n",
            "2018-03-25 14:31:11 (62.8 MB/s) - ‘batch_100_download.sh’ saved [234/234]\n",
            "\n",
            "--2018-03-25 14:31:11--  https://raw.githubusercontent.com/mattheuslee/Deep-Reinforced-Abstractive-Summarization/master/batch_1000/download.sh\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 236 [text/plain]\n",
            "Saving to: ‘batch_1000_download.sh’\n",
            "\n",
            "batch_1000_download 100%[===================>]     236  --.-KB/s    in 0s      \n",
            "\n",
            "2018-03-25 14:31:11 (58.4 MB/s) - ‘batch_1000_download.sh’ saved [236/236]\n",
            "\n",
            "--2018-03-25 14:31:11--  https://raw.githubusercontent.com/mattheuslee/Deep-Reinforced-Abstractive-Summarization/master/cnn_test_stories.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 39020 (38K) [text/plain]\n",
            "Saving to: ‘cnn_test_stories.txt’\n",
            "\n",
            "cnn_test_stories.tx 100%[===================>]  38.11K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2018-03-25 14:31:12 (2.29 MB/s) - ‘cnn_test_stories.txt’ saved [39020/39020]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "KZmAcfWphIK8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Run download script to get batch 100 or 1000"
      ]
    },
    {
      "metadata": {
        "id": "GrSfeNZlhH32",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "output_extras": [
            {
              "item_id": 1
            }
          ],
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1d795bc9-04f8-4423-a0cb-351a0b40c382",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1521988451689,
          "user_tz": -480,
          "elapsed": 134805,
          "user": {
            "displayName": "Mattheus Lee",
            "photoUrl": "//lh4.googleusercontent.com/-73pDOgoEzIs/AAAAAAAAAAI/AAAAAAAAAEQ/awPuz3WAVvI/s50-c-k-no/photo.jpg",
            "userId": "106415181672432219840"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "!bash batch_100_download.sh"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘batch_100’: File exists\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "aZtB32Q0hbGw",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "!bash batch_1000_download.sh"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6puQH7Jpsd7T",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Run to check GPU stats and usage"
      ]
    },
    {
      "metadata": {
        "id": "JlqRoS6tMrQr",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "output_extras": [
            {
              "item_id": 1
            }
          ],
          "base_uri": "https://localhost:8080/",
          "height": 340
        },
        "outputId": "378ff265-c48f-4293-862c-9e6da33b17ed",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1521988616259,
          "user_tz": -480,
          "elapsed": 1170,
          "user": {
            "displayName": "Mattheus Lee",
            "photoUrl": "//lh4.googleusercontent.com/-73pDOgoEzIs/AAAAAAAAAAI/AAAAAAAAAEQ/awPuz3WAVvI/s50-c-k-no/photo.jpg",
            "userId": "106415181672432219840"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# memory footprint support libraries/code\n",
        "!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
        "import psutil\n",
        "import humanize\n",
        "import os\n",
        "import GPUtil as GPU\n",
        "GPUs = GPU.getGPUs()\n",
        "# XXX: only one GPU on Colab and isn’t guaranteed\n",
        "gpu = GPUs[0]\n",
        "process = psutil.Process(os.getpid())\n",
        "print(\"Gen RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ), \" | Proc size: \" + humanize.naturalsize( process.memory_info().rss))\n",
        "print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n",
        "!nvidia-smi"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('Gen RAM Free: 12.2 GB', ' | Proc size: 163.8 MB')\n",
            "GPU RAM Free: 11439MB | Used: 0MB | Util   0% | Total 11439MB\n",
            "Sun Mar 25 14:36:54 2018       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 375.51                 Driver Version: 375.51                    |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 0000:00:04.0     Off |                    0 |\n",
            "| N/A   36C    P8    31W / 149W |      0MiB / 11439MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID  Type  Process name                               Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "lH5n5S1LtYmy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Main Code"
      ]
    },
    {
      "metadata": {
        "id": "n65JdL8fvSMY",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "import nltk\n",
        "from torchtext.data import Field,BucketIterator, TabularDataset\n",
        "from tqdm import tqdm, trange, tnrange, tqdm_notebook\n",
        "import numpy as np\n",
        "from __future__ import print_function\n",
        "\n",
        "from model import Encoder, Decoder"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "g_UJhhA7wDZw",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "TEXT = Field(tokenize=nltk.word_tokenize,use_vocab=True,lower=True, include_lengths=True, batch_first=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8hHHGXUDwGYf",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "MINI_BATCH_SIZE = 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qVI68sfUtdy_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Change path if using different data file**"
      ]
    },
    {
      "metadata": {
        "id": "PniQE5rkYd5O",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "output_extras": [
            {
              "item_id": 1
            }
          ],
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "23ac71d8-643b-4339-d17f-5bdccd77256e",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1521989736652,
          "user_tz": -480,
          "elapsed": 1022,
          "user": {
            "displayName": "Mattheus Lee",
            "photoUrl": "//lh4.googleusercontent.com/-73pDOgoEzIs/AAAAAAAAAAI/AAAAAAAAAEQ/awPuz3WAVvI/s50-c-k-no/photo.jpg",
            "userId": "106415181672432219840"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "attention.py   batch_1000\t       cnn_test_stories.txt  model.pyc\r\n",
            "attention.pyc  batch_1000_download.sh  datalab\t\t     nltk_data\r\n",
            "batch_100      batch_100_download.sh   model.py\t\t     rouge.py\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "BH_Lx1_EwHZo",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "output_extras": [
            {
              "item_id": 465
            },
            {
              "item_id": 466
            },
            {
              "item_id": 467
            }
          ],
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "collapsed": true,
        "outputId": "f97de16b-be90-4faa-ad5c-10951e0aa90f",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1521990094533,
          "user_tz": -480,
          "elapsed": 354574,
          "user": {
            "displayName": "Mattheus Lee",
            "photoUrl": "//lh4.googleusercontent.com/-73pDOgoEzIs/AAAAAAAAAAI/AAAAAAAAAEQ/awPuz3WAVvI/s50-c-k-no/photo.jpg",
            "userId": "106415181672432219840"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "BATCH_SIZE  = 100\n",
        "NUM_BATCHES = 800\n",
        "\n",
        "train_data_batches = []\n",
        "for batch_idx in trange(NUM_BATCHES, desc = \"Loading batches\", unit = \"batch\"):\n",
        "    train_data_batches.append(TabularDataset(path = \"batch_{}/cnn_stories_80000_{}.txt\".format(BATCH_SIZE, batch_idx),\n",
        "                                             format = \"tsv\",\n",
        "                                             fields = [(\"input\", TEXT), (\"target\", TEXT)]))\n",
        "\n",
        "test_data  = TabularDataset(path=\"cnn_test_stories.txt\",\n",
        "                            format='tsv',\n",
        "                            fields=[('input',TEXT),('target',TEXT)])\n",
        "\n",
        "TEXT.build_vocab(test_data, *train_data, min_freq=2)\n",
        "tqdm.write(\"Vocabulary size: {}\".format(len(TEXT.vocab)))\n",
        "\n",
        "batch_train_loaders = [BucketIterator(train_data_batch, \n",
        "                                      batch_size = MINI_BATCH_SIZE,\n",
        "                                      device = -1,\n",
        "                                      sort_key = lambda x: len(x.input),\n",
        "                                      sort_within_batch = True,\n",
        "                                      repeat = False,\n",
        "                                      shuffle = True) for train_data_batch in tqdm(train_data_batches, desc = \"Batch training dataloaders\", unit = \"batch\")]\n",
        "\n",
        "test_loader  = BucketIterator(test_data,\n",
        "                              batch_size = 1, \n",
        "                              device = -1,\n",
        "                              sort_key = lambda x: len(x.input),\n",
        "                              sort_within_batch = True,\n",
        "                              repeat = False,\n",
        "                              shuffle = True)\n",
        "\n",
        "# May be slightly less due to skipping empty stories\n",
        "num_training_examples = np.sum([len(t) for t in train_data])\n",
        "tqdm.write(\"Number of training examples: {}\".format(num_training_examples))\n",
        "tqdm.write(\"Number of testing stories: {}\".format(len(test_data)))\n",
        "\n",
        "HIDDEN = 200\n",
        "EMBED = 100\n",
        "VOCAB_SIZE = len(TEXT.vocab)\n",
        "LR = 0.001"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading batches: 100%|██████████| 800/800 [05:29<00:00,  2.43batch/s]\n",
            "Batch training dataloaders: 100%|██████████| 800/800 [00:00<00:00, 36506.33batch/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Vocabulary size: 163249\n",
            "Number of training examples: 60033\n",
            "Number of testing stories: 10\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "kM0lciHpZcyq",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "encoder = Encoder(VOCAB_SIZE,EMBED,HIDDEN,bidirec=True)\n",
        "decoder = Decoder(VOCAB_SIZE,EMBED,HIDDEN*2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3gtaqdgnQatb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Load/save model"
      ]
    },
    {
      "metadata": {
        "id": "AJInFa63Qd3L",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "output_extras": [
            {
              "item_id": 1
            }
          ],
          "base_uri": "https://localhost:8080/",
          "height": 299
        },
        "outputId": "23730d26-0c91-433a-ddf8-d52778f88204",
        "executionInfo": {
          "status": "error",
          "timestamp": 1521990201367,
          "user_tz": -480,
          "elapsed": 770,
          "user": {
            "displayName": "Mattheus Lee",
            "photoUrl": "//lh4.googleusercontent.com/-73pDOgoEzIs/AAAAAAAAAAI/AAAAAAAAAEQ/awPuz3WAVvI/s50-c-k-no/photo.jpg",
            "userId": "106415181672432219840"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "encoder.load_state_dict(torch.load(\"./encoder.model\"))\n",
        "decoder.load_state_dict(torch.load(\"./decoder.model\"))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IOError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m\u001b[0m",
            "\u001b[0;31mIOError\u001b[0mTraceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-bf55e36a8493>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./encoder.model\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./decoder.model\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/torch/serialization.pyc\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module)\u001b[0m\n\u001b[1;32m    257\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpathlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m         \u001b[0mnew_fd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 259\u001b[0;31m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    260\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIOError\u001b[0m: [Errno 2] No such file or directory: './encoder.model'"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "1n3k9hfsQisO",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "torch.save(encoder.state_dict(), \"./encoder.model\")\n",
        "torch.save(decoder.state_dict(), \"./decoder.model\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hNHLY6lmQX69",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "output_extras": [
            {
              "item_id": 1
            }
          ],
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3d218bd2-eea2-4051-9c7d-3e471d84434a",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1521990393799,
          "user_tz": -480,
          "elapsed": 5394,
          "user": {
            "displayName": "Mattheus Lee",
            "photoUrl": "//lh4.googleusercontent.com/-73pDOgoEzIs/AAAAAAAAAAI/AAAAAAAAAEQ/awPuz3WAVvI/s50-c-k-no/photo.jpg",
            "userId": "106415181672432219840"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "if USE_CUDA:\n",
        "    tqdm.write(\"Using CUDA\")\n",
        "    if torch.cuda.device_count() > 1:\n",
        "        print(\"Using %d devices\" % (torch.cuda.device_count()))\n",
        "        encoder = nn.DataParallel(encoder)\n",
        "        decoder = nn.DataParallel(decoder)\n",
        "    encoder = encoder.cuda()\n",
        "    decoder = decoder.cuda()\n",
        "decoder.embedding = encoder.embedding"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using CUDA\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "dWlm1FyiZhE9",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "loss_function = nn.CrossEntropyLoss(ignore_index=TEXT.vocab.stoi['<pad>'])\n",
        "enc_optim = optim.Adam(encoder.parameters(),lr=LR)\n",
        "dec_optim = optim.Adam(decoder.parameters(),lr=LR)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4pQocA_fyO4R",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Run training"
      ]
    },
    {
      "metadata": {
        "id": "zD33-15GZkD_",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "output_extras": [
            {
              "item_id": 3
            },
            {
              "item_id": 4
            }
          ],
          "base_uri": "https://localhost:8080/",
          "height": 588
        },
        "collapsed": true,
        "outputId": "14591af7-abcf-4db8-fb16-7e4daf03fa55",
        "executionInfo": {
          "status": "error",
          "timestamp": 1521990412963,
          "user_tz": -480,
          "elapsed": 1874,
          "user": {
            "displayName": "Mattheus Lee",
            "photoUrl": "//lh4.googleusercontent.com/-73pDOgoEzIs/AAAAAAAAAAI/AAAAAAAAAEQ/awPuz3WAVvI/s50-c-k-no/photo.jpg",
            "userId": "106415181672432219840"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "NUM_EPOCHS = 1\n",
        "for epoch_idx in trange(NUM_EPOCHS, desc = \"Epochs\", unit = \"epoch\"):\n",
        "    total_loss, total_squared_loss, num_batches = 0.0, 0.0, 0\n",
        "    for batch_train_loader in tqdm(batch_train_loaders, desc = \"Batches\", unit = \"batch\"):\n",
        "        for minibatch in tqdm(batch_train_loader, desc = \"Minibatches\", unit = \"minibatch\"):\n",
        "            inputs,lengths = minibatch.input\n",
        "            targets,_ = minibatch.target\n",
        "            decoding_start = Variable(torch.LongTensor([TEXT.vocab.stoi['<s>']]*targets.size(0))).unsqueeze(1)\n",
        "            if USE_CUDA:\n",
        "                inputs = inputs.cuda()\n",
        "                targets = targets.cuda()\n",
        "                decoding_start = decoding_start.cuda()\n",
        "\n",
        "            encoder.zero_grad()\n",
        "            decoder.zero_grad()\n",
        "            output,hidden = encoder(inputs,lengths.tolist())\n",
        "            score = decoder(decoding_start,hidden,targets.size(1),output,lengths)\n",
        "\n",
        "            loss = loss_function(score,targets.view(-1))\n",
        "            total_loss += loss.data[0]\n",
        "            total_squared_loss += loss.data[0]**2\n",
        "            num_batches += 1\n",
        "            loss.backward()\n",
        "            enc_optim.step()\n",
        "            dec_optim.step()\n",
        "    loss_mean = total_loss / num_batches\n",
        "    loss_variance = (total_squared_loss - (total_loss**2 / num_batches)) / (num_batches - 1)\n",
        "    tqdm.write(\"loss mean: %7.4f, loss variance: %7.4f\" % (loss_mean, loss_variance))"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epochs:   0%|          | 0/1 [00:00<?, ?epoch/s]\n",
            "Batches:   0%|          | 0/800 [00:00<?, ?batch/s]\u001b[A\n",
            "\n",
            "Minibatches:   0%|          | 0/100 [00:00<?, ?minibatch/s]\u001b[A\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\u001b[A\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0mTraceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-5b865717dd7a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0mtotal_squared_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0mnum_batches\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m             \u001b[0menc_optim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0mdec_optim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/torch/autograd/variable.pyc\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m    165\u001b[0m                 \u001b[0mVariable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m         \"\"\"\n\u001b[0;32m--> 167\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/torch/autograd/__init__.pyc\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(variables, grad_variables, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m---> 99\u001b[0;31m         variables, grad_variables, retain_graph)\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: cuda runtime error (2) : out of memory at /pytorch/torch/lib/THC/generic/THCStorage.cu:58"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "o-spPVC-ySDn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Run testing"
      ]
    },
    {
      "metadata": {
        "id": "V5YFERY3xlmn",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "output_extras": [
            {}
          ],
          "base_uri": "https://localhost:8080/",
          "height": 231
        },
        "outputId": "174cea29-1201-489f-9690-ca5c94edd3a4",
        "executionInfo": {
          "status": "error",
          "timestamp": 1521793341757,
          "user_tz": -480,
          "elapsed": 727,
          "user": {
            "displayName": "Mattheus Lee",
            "photoUrl": "//lh4.googleusercontent.com/-73pDOgoEzIs/AAAAAAAAAAI/AAAAAAAAAEQ/awPuz3WAVvI/s50-c-k-no/photo.jpg",
            "userId": "106415181672432219840"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "from rouge import ROUGE\n",
        "from __future__ import print_function\n",
        "rouge = ROUGE()\n",
        "\n",
        "print(TEXT.vocab.)\n",
        "\n",
        "def get_string(summary):\n",
        "    result = \"\"\n",
        "    for idx in summary:\n",
        "        if idx == 1: # <E>\n",
        "            break\n",
        "        elif idx == 0: # <unk>\n",
        "            continue\n",
        "        if idx < len(TEXT.vocab.itos):\n",
        "            result += (TEXT.vocab.itos[idx] + \" \")\n",
        "    return result\n",
        "\n",
        "encoder.eval()\n",
        "decoder.eval()\n",
        "print(\"Testing stories\")\n",
        "for batch in test_loader:\n",
        "    inputs, lengths = batch.input\n",
        "    targets, _ = batch.target\n",
        "    decoding_start = Variable(torch.LongTensor([TEXT.vocab.stoi['<s>']]*targets.size(0))).unsqueeze(1)\n",
        "    if USE_CUDA:\n",
        "        inputs = inputs.cuda()\n",
        "        targets = targets.cuda()\n",
        "        decoding_start = decoding_start.cuda()\n",
        "    \n",
        "    output,hidden = encoder(inputs,lengths.tolist())\n",
        "    score = decoder(decoding_start,hidden,targets.size(1),output,lengths)\n",
        "    \n",
        "    reference_summary = targets.data.cpu().numpy()[0]\n",
        "    generated_summary = [np.argmax(word) for word in score.data.cpu().numpy()[0]]\n",
        "    \n",
        "    reference = get_string(reference_summary)\n",
        "    generated = get_string(generated_summary)\n",
        "    \n",
        "    rouge_score = rouge.score(reference, generated)\n",
        "    \n",
        "    print(\"\\nReference summary:\\n{}\".format(reference))\n",
        "    print(\"\\nGenerated summary:\\n{}\".format(generated))\n",
        "    print(\"\\nROUGE score: {}\\n\".format(rouge_score))\n",
        "\n",
        "print(\"Selection of training stories\")\n",
        "NUM_SHOW_TRAINING_STORIES = 10\n",
        "for i, batch in enumerate(train_loader):\n",
        "    if i == NUM_SHOW_TRAINING_STORIES:\n",
        "        break\n",
        "    inputs, lengths = batch.input\n",
        "    targets, _ = batch.target\n",
        "    decoding_start = Variable(torch.LongTensor([TEXT.vocab.stoi['<s>']]*targets.size(0))).unsqueeze(1)\n",
        "    if USE_CUDA:\n",
        "        inputs = inputs.cuda()\n",
        "        targets = targets.cuda()\n",
        "        decoding_start = decoding_start.cuda()\n",
        "    \n",
        "    output,hidden = encoder(inputs,lengths.tolist())\n",
        "    score = decoder(decoding_start,hidden,targets.size(1),output,lengths)\n",
        "    \n",
        "    reference_summary = targets.data.cpu().numpy()[0]\n",
        "    generated_summary = [np.argmax(word) for word in score.data.cpu().numpy()[0]]\n",
        "    \n",
        "    reference = get_string(reference_summary)\n",
        "    generated = get_string(generated_summary)\n",
        "    \n",
        "    rouge_score = rouge.score(reference, generated)\n",
        "    \n",
        "    print(\"\\nReference summary:\\n{}\".format(reference))\n",
        "    print(\"\\nGenerated summary:\\n{}\".format(generated))\n",
        "    print(\"\\nROUGE score: {}\\n\".format(rouge_score))\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m\u001b[0m",
            "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-82793d96e47a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'encoder' is not defined"
          ]
        }
      ]
    }
  ]
}